{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7cded4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     active environment : base\n",
      "    active env location : C:\\Users\\tj_da\\anaconda3\n",
      "            shell level : 1\n",
      "       user config file : C:\\Users\\tj_da\\.condarc\n",
      " populated config files : C:\\Users\\tj_da\\.condarc\n",
      "          conda version : 23.1.0\n",
      "    conda-build version : 3.23.3\n",
      "         python version : 3.10.9.final.0\n",
      "       virtual packages : __archspec=1=x86_64\n",
      "                          __win=0=0\n",
      "       base environment : C:\\Users\\tj_da\\anaconda3  (writable)\n",
      "      conda av data dir : C:\\Users\\tj_da\\anaconda3\\etc\\conda\n",
      "  conda av metadata url : None\n",
      "           channel URLs : https://repo.anaconda.com/pkgs/main/win-64\n",
      "                          https://repo.anaconda.com/pkgs/main/noarch\n",
      "                          https://repo.anaconda.com/pkgs/r/win-64\n",
      "                          https://repo.anaconda.com/pkgs/r/noarch\n",
      "                          https://repo.anaconda.com/pkgs/msys2/win-64\n",
      "                          https://repo.anaconda.com/pkgs/msys2/noarch\n",
      "          package cache : C:\\Users\\tj_da\\anaconda3\\pkgs\n",
      "                          C:\\Users\\tj_da\\.conda\\pkgs\n",
      "                          C:\\Users\\tj_da\\AppData\\Local\\conda\\conda\\pkgs\n",
      "       envs directories : C:\\Users\\tj_da\\anaconda3\\envs\n",
      "                          C:\\Users\\tj_da\\.conda\\envs\n",
      "                          C:\\Users\\tj_da\\AppData\\Local\\conda\\conda\\envs\n",
      "               platform : win-64\n",
      "             user-agent : conda/23.1.0 requests/2.31.0 CPython/3.10.9 Windows/10 Windows/10.0.19045\n",
      "          administrator : False\n",
      "             netrc file : None\n",
      "           offline mode : False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecd9eb46",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeclarative\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m declarative_base\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext, SparkConf\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# Import libraries needed\n",
    "import csv\n",
    "import sqlite3\n",
    "import json\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Date, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160aa12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_database_and_import_csv(database_name, table_name, csv_file):\n",
    "#     # Read the CSV file to infer column types and names\n",
    "#     df = pd.read_csv(csv_file, nrows=1)\n",
    "\n",
    "#     # Connect to SQLite database\n",
    "#     conn = sqlite3.connect(database_name)\n",
    "#     cursor = conn.cursor()\n",
    "\n",
    "#     # Create a table in the database based on CSV data types\n",
    "#     create_table_query = f'''\n",
    "#         CREATE TABLE {table_name} (\n",
    "#             {', '.join(['{} {}'.format(column, df[column].dtype) for column in df.columns])}\n",
    "#         )\n",
    "#     '''\n",
    "#     cursor.execute(create_table_query)\n",
    "\n",
    "#     # Import CSV data into the table\n",
    "#     df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "\n",
    "#     # Commit the changes and close the connection\n",
    "#     conn.commit()\n",
    "#     conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create SQLite Database, Table, and Import CSV Data\n",
    "\n",
    "# database_name = 'drug_side_effects.db'\n",
    "# table_name = 'drug_table'\n",
    "# csv_file = 'Data/drugs_side_effects_drugs_com.csv'\n",
    "\n",
    "\n",
    "# create_database_and_import_csv(database_name, table_name, csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427a06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_bar_graph(database_name, table_name, column_to_plot):\n",
    "#     # Connect to SQLite database\n",
    "#     conn = sqlite3.connect(database_name)\n",
    "\n",
    "#     # Query data from the database into a DataFrame\n",
    "#     query = f'SELECT * FROM {table_name}'\n",
    "#     df = pd.read_sql_query(query, conn)\n",
    "\n",
    "#     # Close the database connection\n",
    "#     conn.close()\n",
    "\n",
    "#     # Plot a bar graph\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     df[column_to_plot].value_counts().plot(kind='bar', color='skyblue')\n",
    "#     plt.title(f'Bar Graph of {column_to_plot}')\n",
    "#     plt.xlabel(column_to_plot)\n",
    "#     plt.ylabel('Count')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# # Plot a bar graph based on a specific column\n",
    "# column_to_plot = 'column_name'  # Replace with the actual column name you want to plot\n",
    "# plot_bar_graph(database_name, table_name, column_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb6b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/drugs_side_effects_drugs_com.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50db949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364fb27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['no_of_reviews', 'drug_link','medical_condition_url',\"related_drugs\"], axis = 1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe463ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie Plot\n",
    "\n",
    "#figure size\n",
    "sns.set_context('poster', font_scale=0.5)\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "df['rx_otc'].value_counts().plot.pie(autopct='%1.1f%%', textprops={'fontsize':12}).set_title(\"Medication distribution\")\n",
    "\n",
    "df['rx_otc'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e0737e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pie Plot\n",
    "# df['medical_condition'].value_counts().plot.pie(autopct='%1.1f%%', textprops={'fontsize':12}).set_title(\"Target distribution on medical_condition\")\n",
    "top10_medicalCondition_df = df['medical_condition'].value_counts().head(10)\n",
    "top10_medicalCondition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c3700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "top10_medicalCondition_df.plot(kind='bar', color='skyblue')\n",
    "plt.title('Top 10 Medical Conditions')\n",
    "plt.xlabel('Medical Condition')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f68bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your AWS credentials and region\n",
    "aws_access_key_id = aws_access_key_id\n",
    "aws_secret_access_key = aws_secret_access_key \n",
    "region_name = \"us-east-2\"\n",
    "bucket_name = \"capstone-pill-images\"\n",
    "data_prefix = \"data/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30062205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images_from_s3(bucket_name, prefix, local_directory):\n",
    "    conf = SparkConf().setAppName(\"S3ImageDownloader\")\n",
    "    sc = SparkContext(conf=conf)\n",
    "    spark = SparkSession(sc)\n",
    "\n",
    "    # Function to download a single file from S3\n",
    "    def download_file_from_s3(key):\n",
    "        local_path = os.path.join(local_directory, os.path.relpath(key, prefix))\n",
    "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "        s3.download_file(bucket_name, key, local_path)\n",
    "\n",
    "    # List objects from S3\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    keys = [obj['Key'] for obj in response.get('Contents', [])]\n",
    "\n",
    "    # Create an RDD from the list of keys\n",
    "    keys_rdd = sc.parallelize(keys)\n",
    "\n",
    "    # Use Spark's map operation to parallelize the file downloads\n",
    "    keys_rdd.foreach(download_file_from_s3)\n",
    "\n",
    "    # Stop SparkContext\n",
    "    sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
